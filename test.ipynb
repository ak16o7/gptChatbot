{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gpt_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext\n",
    "from langchain import OpenAI\n",
    "import sys\n",
    "import os\n",
    "import signal\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve data to feed the chatbot.\n",
    "# here: e-book from gutenberg.org https://gutenberg.org/files/70678/70678-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function creates a vector index for text documents using OpenAI's GPT language model and saves it as a JSON file.\"\"\"\n",
    "def create_index(path):\n",
    "    max_input = 4096  # max input size\n",
    "    tokens = 256  # max number of tokens to be used\n",
    "    chunk_size = 600  # how much data shall be grabbed at once\n",
    "    max_chunk_overlap = 20\n",
    "\n",
    "    # create prompt\n",
    "    promptHelper = PromptHelper(max_input, tokens, max_chunk_overlap, chunk_size_limit=chunk_size)\n",
    "\n",
    "    # define language model: e.g. LLM Predictor\n",
    "    # choose model: davinci, ada, curie,...\n",
    "    llmPredictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\", max_tokens=tokens))\n",
    "\n",
    "    # load data\n",
    "    docs = SimpleDirectoryReader(path).load_data()\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(llm_predictor=llmPredictor, prompt_helper=promptHelper)\n",
    "    vectorIndex = GPTSimpleVectorIndex.from_documents(documents=docs, service_context=service_context)\n",
    "\n",
    "    #create a vector index\n",
    "    #vector_index = GPTSimpleVectorIndex(docs=docs, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "    #save as json file\n",
    "    vectorIndex.save_to_disk('vector_index.json')\n",
    "    return vectorIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 140564 tokens\n"
     ]
    }
   ],
   "source": [
    "\"\"\"folder name as input\"\"\"\n",
    "vectorIndex = create_index(\"Knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function queries a pre-existing GPT-based vector index of text documents and returns the most similar document(s) to the user's query.\"\"\"\n",
    "def answerMe(vectorIndex):\n",
    "    vIndex = GPTSimpleVectorIndex.load_from_disk(vectorIndex)\n",
    "    signal.signal(signal.SIGINT, lambda signal, frame: sys.exit(0))\n",
    "    while True:\n",
    "        try:\n",
    "            prompt = input('Please ask: ')\n",
    "            response = vIndex.query(prompt, response_mode='compact')\n",
    "            print(f'Prompt: {prompt}')\n",
    "            print(f'Response:{response}')\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Program interrupted.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3790 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 1 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Who\n",
      "Response:\n",
      "\n",
      "Einstein was a scientist and philosopher who was known for his theories on relativity and gravity, as well as his ethical views on science and its role in society. He was also renowned for his postulate of simplicity, which expressed the will of man's mind to arrive at a maximum of result by using a minimum of effort, and to express the greatest sum of experience by using the smallest number of symbols. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3742 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Who is the author of this book?\n",
      "Response:\n",
      "\n",
      "The author of this book is Alexander Moszkowski, who wrote a letter to Albert Einstein asking him to attend one of the informal evenings of the Literary Society at the HÃ´tel Bristol. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3810 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 9 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is this book about, in brief?\n",
      "Response:\n",
      "\n",
      "This book is about Albert Einstein's life, his travels, hobbies, and work habits, and his views on literature, philosophy, art, politics, and other topics. It examines his ideals of internationalism, pacifism, and Zionism, as well as his daily routine of managing a large volume of correspondence and requests for his time. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"load vector index data from json file and prompt queries\"\"\"\n",
    "answerMe(\"vector_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
